{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basics first\n",
    "\n",
    "\n",
    "\n",
    "Pandas is built around two data structures: \n",
    "    - pandas.Series\n",
    "    - pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    3.0\n",
      "2    5.0\n",
      "3    NaN\n",
      "4    6.0\n",
      "5    8.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A Series is a list with labels that can hold any data type\n",
    "## from 10 minutes to pandas (https://pandas.pydata.org/pandas-docs/stable/10min.html#min)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "s = pd.Series([1,3,5,np.nan,6,8])\n",
    "print(s)\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n",
      "               '2013-01-05', '2013-01-06'],\n",
      "              dtype='datetime64[ns]', freq='D')\n",
      "                   A         B         C         D\n",
      "2013-01-01  1.501225 -0.129887 -1.258921 -1.118677\n",
      "2013-01-02  1.529092  0.244244  0.731982 -0.519929\n",
      "2013-01-03  0.183809  0.230549  1.260948 -0.926389\n",
      "2013-01-04  1.148851 -0.676734 -3.173422  1.308157\n",
      "2013-01-05 -0.628584 -0.627767  1.257109 -0.919234\n",
      "2013-01-06  0.997157 -0.374610 -0.009120 -0.332117\n",
      "\n",
      "2013-01-01    1.501225\n",
      "2013-01-02    1.529092\n",
      "2013-01-03    0.183809\n",
      "2013-01-04    1.148851\n",
      "2013-01-05   -0.628584\n",
      "2013-01-06    0.997157\n",
      "Freq: D, Name: A, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## A DataFrame is a two-dimensional labeled data structure that can be subset into Series objects\n",
    "## from 10 minutes to pandas\n",
    "\n",
    "dates = pd.date_range('20130101', periods=6)\n",
    "print(dates)\n",
    "df    = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n",
    "print(df)\n",
    "print()\n",
    "print(df.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file     = \"./10900_Invers_ScanResults.txt\"\n",
    "outDir   = \"./SumStatsVecs\"\n",
    "\n",
    "stats = [\"Hscan_v1.3_H12\", \"pcadapt_3.0.4_ALL_log10p\", \"OutFLANK_0.2_He\", \"LFMM_ridge_0.0_ALL_log10p\",\n",
    "        \"LFMM_lasso_0.0_ALL_log10p\", \"rehh_2.0.2_ALL_log10p\", \"Spearmans_ALL_rho\", \"a_freq_final\", \n",
    "        \"pcadapt_3.0.4_PRUNED_log10p\"]\n",
    "\n",
    "!mkdir -p $outDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective : \n",
    "### Turn '10900_Invers_ScanResults.txt' into a set of 12 feature vectors, one for each class\n",
    "\n",
    "## Steps:\n",
    "1) Read the text file into a dataframe\n",
    "\n",
    "2) Scale all the statistics\n",
    "\n",
    "3) Label each SNP based on region and muttype\n",
    "\n",
    "4) Split the dataframe based on label and print each class to its own file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the text file into a dataframe\n",
    "\n",
    "Pandas has a few convenient function for reading in text files:\n",
    "\n",
    "`df = pd.read_csv(filepath, sep, header,...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Did the file get read correctly?\n",
    "\n",
    "print(featDf.head(3))\n",
    "print(featDf.shape)\n",
    "print(featDf.columns)\n",
    "print(featDf.chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data has a 'keep_loci' column, saying whether an SNP passed the filters or not. We should get rid of the\n",
    "## SNPs that didn't pass\n",
    "\n",
    "featDf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'group_by' function\n",
    "\n",
    "`DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, observed=False, **kwargs)`\n",
    "\n",
    "group_by is a very useful function. You can call it on your dataframe with a function or label to group it by to get a group_by object that consists of the dataframe split up into different dataframes based on the function or label. You can access groups with get_group:\n",
    "\n",
    "`GroupBy.get_group(name, obj=None)`\n",
    "\n",
    "name is the name of the group, obj is the group_by object to take it from (default is the object it was called on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop function\n",
    "\n",
    "`DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')`\n",
    "\n",
    "You can call .drop on a datframe to remove data from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNPs on chromosome 9 have variable recombination, meaning this \n",
    "## chromosome is not the same in every simulation and it is hard to\n",
    "## classify. How can we just remove this from our data?\n",
    "\n",
    "## The group_by() function is well suited to the task\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the statistics\n",
    "\n",
    "Machine learning features should be scaled before they are given to a classifier. In this case, the scaling we want to do works like this: \n",
    "\n",
    "Take the statistic in one column. If it is negative at any of the SNPs, add the smallest value of the statistic to every value in the column. Now, take the sum of the column and divide each value in the column by that sum. Repeat for each column\n",
    "\n",
    "All features are now between 0 and 1.\n",
    "\n",
    "I'll give you a scale stats function I wrote to do the math with a single column, but we have to figure out how to scale every column with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleStats(statSeries):\n",
    "    #### some of the values for pcadaptlog10p were 'Inf'. This breaks some of the math, so I replaced the values\n",
    "    #### with a very large log10p value of 400, which represents an p-value extremely close to 0 and lower than \n",
    "    #### any of the non-Inf p-values\n",
    "    statSeries.replace('Inf', 400, inplace = True)\n",
    "    statSeries = pd.to_numeric(statSeries, errors = 'coerce')\n",
    "    \n",
    "    # if there are any negative values, scale by addition first\n",
    "    minStat = statSeries.min()\n",
    "    if minStat < 0: \n",
    "        statSeries = statSeries + minStat\n",
    "    \n",
    "    # scale by dividing values by the sum\n",
    "    if statSeries.sum() != 0: \n",
    "        return(statSeries.divide(statSeries.sum(), fill_value = 0))\n",
    "    else:\n",
    "        return(statSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The apply function\n",
    "\n",
    "`DataFrame.apply(func, axis=0, broadcast=None, raw=False, reduce=None, result_type=None, args=(), **kwds)`\n",
    "\n",
    "func -- function to apply\n",
    "\n",
    "axis = 0 applies by column, axis = 1 applies by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How can we scale every without using a loop (too slow)?\n",
    "## Scaling columns such as 'chrom' or 'pos' doesn't make sense, how do we only scale the columns we want?\n",
    "\n",
    "## The 'apply' function allows us to do it in a single line\n",
    "\n",
    "scaledFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the SNPs\n",
    "\n",
    "The snps are labeled based on their position and their muttype. We can easily write a function to take a SNPs position and muttype and return a label, but how can we 'apply' this function if it takes variables in two different columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Labels\n",
    "\n",
    "possible muttypes:\n",
    "\n",
    " - neutral\n",
    " - QTN         : can be either large effect (>.20 of variation in phenotype) or small effect (<.20 of variation)\n",
    " - deleterious : mutation that negatively effects fitness\n",
    " - sweep       : mutation that has become fixed and is expected to show evidence of a selective sweep around it\n",
    "\n",
    "possible regions:\n",
    "\n",
    " - Background selection : any SNP in the 10,000bp region where deleterious mutations occurred \n",
    " - Near Selective Sweep : within 1,000bp of the selective sweep\n",
    " - Far Selective Sweep  : 1,000-2,000bp from the selective sweep\n",
    " - large QTN linked     : within 200bp of a QTN of large effect\n",
    " - small QTN linked     : within 200bp of a QTN of small effect\n",
    " - inversion            : in an inversion\n",
    " - low recombination    : in a region of low recombination\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLabel(pos, muttype):\n",
    "    # 1 = neut, 2 = QTN, 3 = delet, 4 = sweep\n",
    "    muttypes = {\"MT=1\" : \"neut\", \n",
    "                \"MT=2\" : \"QTN\",\n",
    "                \"MT=3\" : \"delet\",\n",
    "                \"MT=4\" : \"sweep\",\n",
    "                \"MT=5\" : \"neut\"}         ### MT=5 is a artifact from SLiM to preserve the inversion\n",
    "    try:\n",
    "        mtLabel = muttypes[muttype]\n",
    "    except KeyError:\n",
    "        warnings.warn(\"Unknown muttype \" + muttype)\n",
    "        mtLabel = \"INVALID\"\n",
    "    \n",
    "    pos = float(pos)\n",
    "    if  200001 <= pos <= 230000 or  270001 <= pos <= 280000:\n",
    "        region = \"BS\"\n",
    "    elif 174000 <= pos <= 176000:\n",
    "        region = \"NearSS\"\n",
    "    elif 173000 <= pos <= 17399 or 176001 <= pos <= 177000:\n",
    "        region = \"FarSS\"\n",
    "    elif 320000 <= pos <= 330000:\n",
    "        region = \"invers\"\n",
    "    elif 370000 <= pos <= 380000:\n",
    "        region = \"lowRC\"\n",
    "    else:\n",
    "        region = \"neutral\"\n",
    "    return \"MT=\" + mtLabel + \"_R=\" + region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'insert' function\n",
    "\n",
    "`DataFrame.insert(loc, column, value, allow_duplicates=False)`\n",
    "\n",
    "Pretty simple function that inserts the 'value' at the given 'loc' and names the new column 'column'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What's the best way to apply a function to two columns of a data frame?\n",
    "\n",
    "## Answer from stack overflow:\n",
    "## rewrite the function to take a pandas series. Apply the function row wise\n",
    "## (https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe)\n",
    "\n",
    "\n",
    "print(scaledFeatures.shape)\n",
    "print(scaledFeatures.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to add some more labels -- MT=2 means a QTN but does not distinguish between large and small QTNS\n",
    "## In addition, there is no muttype for 'linked to a large QTN'. We are going to need the position column to \n",
    "# locate the linked alleles and the proportion column to differentiate the large and small QTNs\n",
    "\n",
    "\n",
    "# add pos and prop back in to locate QTNs of large and small effect\n",
    "scaledFeatures.insert(loc = 0, column = 'pos', value = features['pos'].astype(\"float\"))\n",
    "scaledFeatures.insert(loc = 0, column = 'prop', value = pd.to_numeric(features['prop'], errors = \"coerce\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loc function:\n",
    "\n",
    "`DataFrame.loc[[\"row_label_1\", \"row_label_2], 'col1':]`\n",
    "\n",
    "loc accesses a group of rows or columns or both from a dataframe. The indexing operator ([]) cannot select rows and columns at the same time and can give unexpected results when performing more complex operations. loc is a more consistent and explicit way to select data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'isin' function\n",
    "\n",
    "`DataFrame.isin(values)`\n",
    "\n",
    "Returns a boolean DataFrame showing whether each element in the dataframe is contained in values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have the proportions, we need to actually filter through them find the large and small QTNs\n",
    "## The rule is, < 20 % proportion is a small QTN and > 20% proportion is a large QTN. This only applies to \n",
    "## SNPs marked as QTNs in the first place.\n",
    "\n",
    "## We can do this with no additional functions, just some more complicated subsetting\n",
    "largeQTNs = scaledFeatures[]\n",
    "smallQTNs = scaledFeatures[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the labels --  use the 'loc' function and the 'isin' function\n",
    "scaledFeatures.loc[] = 'MT=lgQTN_R=lgQTNlink'\n",
    "scaledFeatures.loc[] = 'MT=smQTN_R=smQTNlink'\n",
    "\n",
    "## check the original data frame was changed\n",
    "print(scaledFeatures[scaledFeatures.pos.isin(smallQTNs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'between' function\n",
    "\n",
    "`Series.between(left, right, inclusive=True)`\n",
    "\n",
    "Takes a series, returns a boolean series indicating whether each element in the series is between 'left' and 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to label all the QTN linked SNPs --- these are defined as any SNP within 200bp of a QTN.\n",
    "## Small and large QTN linked SNPs are labeled differently, and an SNP that is within 200bp of a large and a small\n",
    "## QTN should be labeled as large QTN linked\n",
    "\n",
    "for site in smallQTNs:\n",
    "    lower = site - 200\n",
    "    upper = site + 200\n",
    "    ### use loc to access and change the label of all SNPs between lower and upper, EXCEPT the QTN itself\n",
    "    scaledFeatures.loc[] = 'MT=neut_R=smQTNlink'\n",
    "    \n",
    "for site in largeQTNs:\n",
    "    lower = site - 200\n",
    "    upper = site + 200\n",
    "    ### again, access and change the label using loc\n",
    "    scaledFeatures.loc[] = 'MT=neut_R=smQTNlink'\n",
    "    \n",
    "## Can you think of a way to do this that doesn't use a for loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the data to separate files, based on the class\n",
    "\n",
    "Almost finished! Let's drop the columns that don't contain statistics, then split up the dataframe based on class_label (sounds like another job for groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the 'drop' function to remove the 'pos' and 'prop' columns\n",
    "\n",
    "print(scaledFeatures.head())\n",
    "print(scaledFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use 'groupby' to group the columns based on class label\n",
    "\n",
    "print(labelGrouped.groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'to_csv' function\n",
    "\n",
    "`DataFrame.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression=None, quoting=None, quotechar='\"', line_terminator='\\n', chunksize=None, tupleize_cols=None, date_format=None, doublequote=True, escapechar=None, decimal='.')`\n",
    "\n",
    "Call on a dataframe object to print the dataframe to a csv file. Ex:\n",
    "\n",
    "`df.to_csv(filename, sep = \" \", index = False, header = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## go through each group, generate a file name, and use the pandas function group.to_csv to print to file\n",
    "\n",
    "# loop through the groupby object:\n",
    "        outfile     = outDir + \"/\" + name + \".fvec\"\n",
    "        outfile     = outfile.replace(\"=\", \"-\")         ## unix doesn't like having '=' in file names\n",
    "        \n",
    "        ## use the .to_csv function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $outDir\n",
    "print(\"\\n\\n\")\n",
    "!head $outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interested in learning more?\n",
    "\n",
    "There are tons of great online resources for learning pandas!\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/tutorials.html\n",
    "\n",
    "There's also been quite a lot written on Stack Overflow about pandas, if you're not sure how to do something search it and you'll probably find someone with the exact same question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
