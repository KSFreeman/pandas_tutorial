{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file     = \"./10900_Invers_ScanResults.txt\"\n",
    "outDir   = \"./SumStatsVecs\"\n",
    "\n",
    "stats = [\"Hscan_v1.3_H12\", \"pcadapt_3.0.4_ALL_log10p\", \"OutFLANK_0.2_He\", \"LFMM_ridge_0.0_ALL_log10p\",\n",
    "        \"LFMM_lasso_0.0_ALL_log10p\", \"rehh_2.0.2_ALL_log10p\", \"Spearmans_ALL_rho\", \"a_freq_final\", \n",
    "        \"pcadapt_3.0.4_PRUNED_log10p\"]\n",
    "\n",
    "!mkdir -p $outDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective : \n",
    "### Turn '10900_Invers_ScanResults.txt' into a set of 12 feature vectors, one for each class\n",
    "\n",
    "## Steps:\n",
    "1) Read the text file into a dataframe\n",
    "\n",
    "2) Scale all the statistics\n",
    "\n",
    "3) Label each SNP based on region and muttype\n",
    "\n",
    "4) Split the dataframe based on label and print each class to its own file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the text file into a dataframe\n",
    "\n",
    "Pandas has a few convenient function for reading in text files:\n",
    "\n",
    "df = pd.read_csv(filepath, sep, header,...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vcf_ord  pos  chrom  a_freq_old muttype            unique  for_relatedness  \\\n",
      "0        1   62      1      0.3005    MT=1   10900_62_MT=1_1             True   \n",
      "1        4  392      1      0.2035    MT=1  10900_392_MT=1_4            False   \n",
      "2        6  445      1      0.0720    MT=1  10900_445_MT=1_6            False   \n",
      "\n",
      "   a_freq_final  keep_loci  simID ... rehh_2.0.2_ALL_iHS  \\\n",
      "0      0.299197       True  10900 ...                NaN   \n",
      "1      0.203313       True  10900 ...                NaN   \n",
      "2      0.072289       True  10900 ...                NaN   \n",
      "\n",
      "   rehh_2.0.2_ALL_log10p  Spearmans_ALL_rho  selCoef  originGen  freq_old  \\\n",
      "0                    NaN           0.050616      NaN        NaN       NaN   \n",
      "1                    NaN          -0.025850      NaN        NaN       NaN   \n",
      "2                    NaN          -0.031636      NaN        NaN       NaN   \n",
      "\n",
      "   freq_final  pa2  prop  He  \n",
      "0         NaN  NaN   NaN NaN  \n",
      "1         NaN  NaN   NaN NaN  \n",
      "2         NaN  NaN   NaN NaN  \n",
      "\n",
      "[3 rows x 37 columns]\n",
      "(6927, 37)\n",
      "Index(['vcf_ord', 'pos', 'chrom', 'a_freq_old', 'muttype', 'unique',\n",
      "       'for_relatedness', 'a_freq_final', 'keep_loci', 'simID', 'quasi_indep',\n",
      "       'Hscan_v1.3_H12', 'pca_ALL_PC1_loadings', 'pca_ALL_PC2_loadings',\n",
      "       'pca_ALL_PC3_loadings', 'pca_PRUNED_PC1_loadings',\n",
      "       'pca_PRUNED_PC2_loadings', 'pcadapt_3.0.4_ALL_chisq',\n",
      "       'pcadapt_3.0.4_ALL_log10p', 'pcadapt_3.0.4_PRUNED_log10p',\n",
      "       'OutFLANK_0.2_FST', 'OutFLANK_0.2_He', 'OutFLANK_0.2_ALL_log10p',\n",
      "       'OutFLANK_0.2_PRUNED_log10p', 'LFMM_ridge_0.0_ALL_log10p',\n",
      "       'LFMM_lasso_0.0_ALL_log10p', 'CHR', 'rehh_2.0.2_ALL_iHS',\n",
      "       'rehh_2.0.2_ALL_log10p', 'Spearmans_ALL_rho', 'selCoef', 'originGen',\n",
      "       'freq_old', 'freq_final', 'pa2', 'prop', 'He'],\n",
      "      dtype='object')\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "5        1\n",
      "6        1\n",
      "7        1\n",
      "8        1\n",
      "9        1\n",
      "10       1\n",
      "11       1\n",
      "12       1\n",
      "13       1\n",
      "14       1\n",
      "15       1\n",
      "16       1\n",
      "17       1\n",
      "18       1\n",
      "19       1\n",
      "20       1\n",
      "21       1\n",
      "22       1\n",
      "23       1\n",
      "24       1\n",
      "25       1\n",
      "26       1\n",
      "27       1\n",
      "28       1\n",
      "29       1\n",
      "        ..\n",
      "6897    10\n",
      "6898    10\n",
      "6899    10\n",
      "6900    10\n",
      "6901    10\n",
      "6902    10\n",
      "6903    10\n",
      "6904    10\n",
      "6905    10\n",
      "6906    10\n",
      "6907    10\n",
      "6908    10\n",
      "6909    10\n",
      "6910    10\n",
      "6911    10\n",
      "6912    10\n",
      "6913    10\n",
      "6914    10\n",
      "6915    10\n",
      "6916    10\n",
      "6917    10\n",
      "6918    10\n",
      "6919    10\n",
      "6920    10\n",
      "6921    10\n",
      "6922    10\n",
      "6923    10\n",
      "6924    10\n",
      "6925    10\n",
      "6926    10\n",
      "Name: chrom, Length: 6927, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Did the file get read correctly?\n",
    "\n",
    "print(featDf.head(3))\n",
    "print(featDf.shape)\n",
    "print(featDf.columns)\n",
    "print(featDf.chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6909, 37)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The data has a 'keep_loci' column, saying whether an SNP passed the filters or not. We should get rid of the\n",
    "## SNPs that didn't pass\n",
    "\n",
    "featDf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'group_by' function\n",
    "\n",
    "`DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, observed=False, **kwargs)`\n",
    "\n",
    "group_by is a very useful function. You can call it on your dataframe with a function or label to group it by to get a group_by object that consists of the dataframe split up into different dataframes based on the function or label. You can access groups with get_group:\n",
    "\n",
    "`GroupBy.get_group(name, obj=None)`\n",
    "\n",
    "name is the name of the group, obj is the group_by object to take it from (default is the object it was called on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6220, 37)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SNPs on chromosome 9 have variable recombination, meaning this \n",
    "## chromosome is not the same in every simulation and it is hard to\n",
    "## classify. How can we just remove this from our data?\n",
    "\n",
    "## The group_by() function is well suited to the task\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the statistics\n",
    "\n",
    "Machine learning features should be scaled before they are given to a classifier. In this case, the scaling we want to do works like this: \n",
    "\n",
    "Take the statistic in one column. If it is negative at any of the SNPs, add the smallest value of the statistic to every value in the column. Now, take the sum of the column and divide each value in the column by that sum. Repeat for each column\n",
    "\n",
    "All features are now between 0 and 1.\n",
    "\n",
    "I'll give you a scale stats function I wrote to do the math with a single column, but we have to figure out how to scale every column with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleStats(statSeries):\n",
    "    #### some of the values for pcadaptlog10p were 'Inf'. This breaks some of the math, so I replaced the values\n",
    "    #### with a very large log10p value of 400, which represents an p-value extremely close to 0 and lower than \n",
    "    #### any of the non-Inf p-values\n",
    "    statSeries.replace('Inf', 400, inplace = True)\n",
    "    statSeries = pd.to_numeric(statSeries, errors = 'coerce')\n",
    "    \n",
    "    # if there are any negative values, scale by addition first\n",
    "    minStat = statSeries.min()\n",
    "    if minStat < 0: \n",
    "        statSeries = statSeries + minStat\n",
    "    \n",
    "    # scale by dividing values by the sum\n",
    "    if statSeries.sum() != 0: \n",
    "        return(statSeries.divide(statSeries.sum(), fill_value = 0))\n",
    "    else:\n",
    "        return(statSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The apply function\n",
    "\n",
    "`DataFrame.apply(func, axis=0, broadcast=None, raw=False, reduce=None, result_type=None, args=(), **kwds)`\n",
    "\n",
    "func -- function to apply\n",
    "\n",
    "axis = 0 applies by column, axis = 1 applies by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6220, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How can we scale every without using a loop (too slow)?\n",
    "## Scaling columns such as 'chrom' or 'pos' doesn't make sense, how do we only scale the columns we want?\n",
    "\n",
    "## The 'apply' function allows us to do it in a single line\n",
    "\n",
    "scaledFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the SNPs\n",
    "\n",
    "The snps are labeled based on their position and their muttype. We can easily write a function to take a SNPs position and muttype and return a label, but how can we 'apply' this function if it takes variables in two different columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Labels\n",
    "\n",
    "possible muttypes:\n",
    "\n",
    " - neutral\n",
    " - QTN         : can be either large effect (>.20 of variation in phenotype) or small effect (<.20 of variation)\n",
    " - deleterious : mutation that negatively effects fitness\n",
    " - sweep       : mutation that has become fixed and is expected to show evidence of a selective sweep around it\n",
    "\n",
    "possible regions:\n",
    "\n",
    " - Background selection : any SNP in the 10,000bp region where deleterious mutations occurred \n",
    " - Near Selective Sweep : within 1,000bp of the selective sweep\n",
    " - Far Selective Sweep  : 1,000-2,000bp from the selective sweep\n",
    " - large QTN linked     : within 200bp of a QTN of large effect\n",
    " - small QTN linked     : within 200bp of a QTN of small effect\n",
    " - inversion            : in an inversion\n",
    " - low recombination    : in a region of low recombination\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLabel(pos, muttype):\n",
    "    # 1 = neut, 2 = QTN, 3 = delet, 4 = sweep\n",
    "    muttypes = {\"MT=1\" : \"neut\", \n",
    "                \"MT=2\" : \"QTN\",\n",
    "                \"MT=3\" : \"delet\",\n",
    "                \"MT=4\" : \"sweep\",\n",
    "                \"MT=5\" : \"neut\"}         ### MT=5 is a artifact from SLiM to preserve the inversion\n",
    "    try:\n",
    "        mtLabel = muttypes[muttype]\n",
    "    except KeyError:\n",
    "        warnings.warn(\"Unknown muttype \" + muttype)\n",
    "        mtLabel = \"INVALID\"\n",
    "    \n",
    "    pos = float(pos)\n",
    "    if  200001 <= pos <= 230000 or  270001 <= pos <= 280000:\n",
    "        region = \"BS\"\n",
    "    elif 174000 <= pos <= 176000:\n",
    "        region = \"NearSS\"\n",
    "    elif 173000 <= pos <= 17399 or 176001 <= pos <= 177000:\n",
    "        region = \"FarSS\"\n",
    "    elif 320000 <= pos <= 330000:\n",
    "        region = \"invers\"\n",
    "    elif 370000 <= pos <= 380000:\n",
    "        region = \"lowRC\"\n",
    "    else:\n",
    "        region = \"neutral\"\n",
    "    return \"MT=\" + mtLabel + \"_R=\" + region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'insert' function\n",
    "\n",
    "`DataFrame.insert(loc, column, value, allow_duplicates=False)`\n",
    "\n",
    "Pretty simple function that inserts the 'value' at the given 'loc' and names the new column 'column'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6220, 10)\n",
      "          classLabel  Hscan_v1.3_H12  pcadapt_3.0.4_ALL_log10p  \\\n",
      "0  MT=neut_R=neutral             0.0                       0.0   \n",
      "1  MT=neut_R=neutral             0.0                       0.0   \n",
      "2  MT=neut_R=neutral             0.0                       0.0   \n",
      "3  MT=neut_R=neutral             0.0                       0.0   \n",
      "4  MT=neut_R=neutral             0.0                       0.0   \n",
      "\n",
      "   OutFLANK_0.2_He  LFMM_ridge_0.0_ALL_log10p  LFMM_lasso_0.0_ALL_log10p  \\\n",
      "0         0.000313                   0.000121                   0.000008   \n",
      "1         0.000242                   0.000142                   0.000120   \n",
      "2         0.000100                   0.000089                   0.000040   \n",
      "3         0.000017                   0.000085                   0.000312   \n",
      "4         0.000022                   0.000533                   0.000178   \n",
      "\n",
      "   rehh_2.0.2_ALL_log10p  Spearmans_ALL_rho  a_freq_final  \\\n",
      "0                    0.0           0.000137      0.000223   \n",
      "1                    0.0           0.000172      0.000151   \n",
      "2                    0.0           0.000175      0.000054   \n",
      "3                    0.0           0.000136      0.000009   \n",
      "4                    0.0           0.000223      0.000011   \n",
      "\n",
      "   pcadapt_3.0.4_PRUNED_log10p  \n",
      "0                     0.000201  \n",
      "1                     0.000061  \n",
      "2                     0.000083  \n",
      "3                     0.000020  \n",
      "4                     0.000516  \n"
     ]
    }
   ],
   "source": [
    "## What's the best way to apply a function to two columns of a data frame?\n",
    "\n",
    "## Answer from stack overflow:\n",
    "## rewrite the function to take a pandas series. Apply the function row wise\n",
    "## (https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe)\n",
    "\n",
    "\n",
    "print(scaledFeatures.shape)\n",
    "print(scaledFeatures.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to add some more labels -- MT=2 means a QTN but does not distinguish between large and small QTNS\n",
    "## In addition, there is no muttype for 'linked to a large QTN'. We are going to need the position column to \n",
    "# locate the linked alleles and the proportion column to differentiate the large and small QTNs\n",
    "\n",
    "\n",
    "# add pos and prop back in to locate QTNs of large and small effect\n",
    "scaledFeatures.insert(loc = 0, column = 'pos', value = features['pos'].astype(\"float\"))\n",
    "scaledFeatures.insert(loc = 0, column = 'prop', value = pd.to_numeric(features['prop'], errors = \"coerce\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loc function:\n",
    "\n",
    "`DataFrame.loc[]`\n",
    "\n",
    "loc accesses a group of rows or columns directly from a dataframe. You can modify the loc object directly, it is not a copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'isin' function\n",
    "\n",
    "`DataFrame.isin(values)`\n",
    "\n",
    "Returns a boolean DataFrame showing whether each element in the dataframe is contained in values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have the proportions, we need to actually filter through them find the large and small QTNs\n",
    "## The rule is, < 20 % proportion is a small QTN and > 20% proportion is a large QTN. This only applies to \n",
    "## SNPs marked as QTNs in the first place.\n",
    "\n",
    "## We can do this with no additional functions, just some more complicated subsetting\n",
    "\n",
    "\n",
    "## update the labels --  use the 'loc' function and the 'isin' function\n",
    "## loc function avoids chain indexing, which is important when setting values in the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'between' function\n",
    "\n",
    "`Series.between(left, right, inclusive=True)`\n",
    "\n",
    "Takes a series, returns a boolean series indicating whether each element in the series is between 'left' and 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to label all the QTN linked SNPs --- these are defined as any SNP within 200bp of a QTN.\n",
    "## Small and large QTN linked SNPs are labeled differently, and an SNP that is within 200bp of a large and a small\n",
    "## QTN should be labeled as large QTN linked\n",
    "\n",
    "for site in smallQTNs:\n",
    "    lower = site - 200\n",
    "    upper = site + 200\n",
    "    ### use loc to access and change the label of all SNPs between lower and upper, EXCEPT the QTN itself\n",
    "    \n",
    "for site in largeQTNs:\n",
    "    lower = site - 200\n",
    "    upper = site + 200\n",
    "    ### again, access and change the label using loc\n",
    "        \n",
    "## Can you think of a way to do this that doesn't use a for loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the data to separate files, based on the class\n",
    "\n",
    "Almost finished! Let's drop the columns that don't contain statistics, then split up the dataframe based on class_label (sounds like another job for groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          classLabel  Hscan_v1.3_H12  pcadapt_3.0.4_ALL_log10p  \\\n",
      "0  MT=neut_R=neutral             0.0                       0.0   \n",
      "1  MT=neut_R=neutral             0.0                       0.0   \n",
      "2  MT=neut_R=neutral             0.0                       0.0   \n",
      "3  MT=neut_R=neutral             0.0                       0.0   \n",
      "4  MT=neut_R=neutral             0.0                       0.0   \n",
      "\n",
      "   OutFLANK_0.2_He  LFMM_ridge_0.0_ALL_log10p  LFMM_lasso_0.0_ALL_log10p  \\\n",
      "0         0.000313                   0.000121                   0.000008   \n",
      "1         0.000242                   0.000142                   0.000120   \n",
      "2         0.000100                   0.000089                   0.000040   \n",
      "3         0.000017                   0.000085                   0.000312   \n",
      "4         0.000022                   0.000533                   0.000178   \n",
      "\n",
      "   rehh_2.0.2_ALL_log10p  Spearmans_ALL_rho  a_freq_final  \\\n",
      "0                    0.0           0.000137      0.000223   \n",
      "1                    0.0           0.000172      0.000151   \n",
      "2                    0.0           0.000175      0.000054   \n",
      "3                    0.0           0.000136      0.000009   \n",
      "4                    0.0           0.000223      0.000011   \n",
      "\n",
      "   pcadapt_3.0.4_PRUNED_log10p  \n",
      "0                     0.000201  \n",
      "1                     0.000061  \n",
      "2                     0.000083  \n",
      "3                     0.000020  \n",
      "4                     0.000516  \n",
      "(6220, 10)\n"
     ]
    }
   ],
   "source": [
    "## Use the 'drop' function to remove the 'pos' and 'prop' columns\n",
    "\n",
    "print(scaledFeatures.head())\n",
    "print(scaledFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x7fc1875a64e0>\n"
     ]
    }
   ],
   "source": [
    "## use 'groupby' to group the columns based on class label\n",
    "\n",
    "print(labelGrouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'to_csv' function\n",
    "\n",
    "`DataFrame.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression=None, quoting=None, quotechar='\"', line_terminator='\\n', chunksize=None, tupleize_cols=None, date_format=None, doublequote=True, escapechar=None, decimal='.')`\n",
    "\n",
    "Call on a dataframe object to print the dataframe to a csv file. Ex:\n",
    "\n",
    "`df.to_csv(filename, sep = \" \", index = False, header = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT-delet_R-BS.fvec\t   MT-neut_R-invers.fvec     MT-neut_R-neutral.fvec\n",
      "MT-lgQTN_R-lgQTNlink.fvec  MT-neut_R-lgQTNlink.fvec  MT-neut_R-smQTNlink.fvec\n",
      "MT-neut_R-BS.fvec\t   MT-neut_R-lowRC.fvec      MT-smQTN_R-smQTNlink.fvec\n",
      "MT-neut_R-FarSS.fvec\t   MT-neut_R-NearSS.fvec     MT-sweep_R-NearSS.fvec\n",
      "\n",
      "\n",
      "\n",
      "classLabel\tHscan_v1.3_H12\tpcadapt_3.0.4_ALL_log10p\tOutFLANK_0.2_He\tLFMM_ridge_0.0_ALL_log10p\tLFMM_lasso_0.0_ALL_log10p\trehh_2.0.2_ALL_log10p\tSpearmans_ALL_rho\ta_freq_final\tpcadapt_3.0.4_PRUNED_log10p\n",
      "MT=sweep_R=NearSS\t0.0004853543746082687\t0.0\t5.544337452407637e-05\t6.855550384918943e-05\t9.133878596663905e-05\t0.0\t0.000126218561145908\t0.0007156107017992282\t0.00014585300829671226\n"
     ]
    }
   ],
   "source": [
    "## go through each group, generate a file name, and use the pandas function group.to_csv to print to file\n",
    "\n",
    "# loop through the groupby object:\n",
    "        outfile     = outDir + \"/\" + name + \".fvec\"\n",
    "        outfile     = outfile.replace(\"=\", \"-\")         ## unix doesn't like havign '=' in file names\n",
    "        \n",
    "        with open(outfile, 'a') as f:\n",
    "            ## apply the .to_csv function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $outDir\n",
    "print(\"\\n\\n\")\n",
    "!head $outfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
